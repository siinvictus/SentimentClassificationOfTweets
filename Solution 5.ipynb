{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 5 - Pipeline 1 and 2 on Prep 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from  sklearn.metrics  import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from  sklearn.metrics  import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from bs4 import BeautifulSoup\n",
    "dev_df = pd.read_csv(\"C:/Users/LENOVO/Downloads/DSL2122_january_dataset/DSL2122_january_dataset/development.csv\")\n",
    "eval_df = pd.read_csv(\"C:/Users/LENOVO/Downloads/DSL2122_january_dataset/DSL2122_january_dataset/evaluation.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 5 -  with PorterStemmer including user and date attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORK ON DUPLICATES => remove the duplicates\n",
    "dev_df.drop_duplicates(subset = 'text', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns that we won't use\n",
    "dev_df.drop(['ids','flag'], axis=1, inplace=True)\n",
    "eval_df.drop(['ids', 'flag'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of a dictionary of negations that splits abbrevations onto their full format.\n",
    "#creation of the cleaner function in order to preprocess only the text feature of both development and evaluation set\n",
    "#the function itself handles the HTML decoding, the @, the URLs, the uppercase letters, the negations and tokenizes the tweets\n",
    "#eliminating the punctuations\n",
    "\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def cleaner(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    soup = BeautifulSoup(tweet, 'html.parser')\n",
    "    tweet = soup.get_text()\n",
    "    tweet = re.sub('@[A-Za-z0-9]+',\"\",tweet)\n",
    "    tweet = re.sub(r'https?://[^ ]+', \"\", tweet)\n",
    "    tweet = re.sub(r'www.[^ ]+', \"\", tweet)\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n",
    "    tweet_lower = tweet.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], tweet_lower)\n",
    "    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    tweet_tokens = tokenizer.tokenize(neg_handled)\n",
    "\n",
    "\n",
    " \n",
    "    theTweet=''\n",
    "\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in string.punctuation):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            theTweet= theTweet+ \" \" + stem_word\n",
    "    \n",
    "    return theTweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1213: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "#the attribute 'date' in both development set and evaluation set has type <object>\n",
    "#the pandas library transform the date object in datetime64[ns] which allows us to extrapolate the part of the datetime\n",
    "#that we require with the pre-defined pandas functions\n",
    "\n",
    "dev_df['date']=pd.to_datetime(dev_df['date'])\n",
    "eval_df['date']=pd.to_datetime(eval_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction only of the hour from the datetime which is in format (hour:min:sec)\n",
    "dev_df['date_hour']=dev_df['date'].dt.hour\n",
    "eval_df['date_hour']=eval_df['date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction of the date in the format (year-month-day)\n",
    "dev_df['date']=dev_df['date'].dt.date\n",
    "eval_df['date']=eval_df['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"Cookies \" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#cleaner function applied for cleaning the text column of the dev dataset\n",
    "testing = dev_df.text\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(cleaner(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaner function applied for cleaning the text column of the eval dataset\n",
    "eval_testing = eval_df.text\n",
    "eval_test_result = []\n",
    "for t in eval_testing:\n",
    "    eval_test_result.append(cleaner(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the clean_text columns in both the development and evaluation dataframe\n",
    "dev_df['clean_text'] = test_result\n",
    "eval_df['clean_text'] = eval_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.drop(['text'], axis=1, inplace=True)\n",
    "eval_df.drop(['text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of a new column merging the attributes: text, date, hour and user all in the string format\n",
    "dev_df['total'] = dev_df['date'].astype(str) + \" \" + dev_df['date_hour'].astype(str) + \" \" + dev_df['clean_text'] + \" \" + dev_df['user']\n",
    "eval_df['total'] = eval_df['date'].astype(str) + \" \" + eval_df['date_hour'].astype(str) + \" \" + eval_df['clean_text'] + \" \" + eval_df['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>date_hour</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>Killandra</td>\n",
       "      <td>1</td>\n",
       "      <td>ye talk help a lot go through it there s no j...</td>\n",
       "      <td>2009-05-18 1  ye talk help a lot go through it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-05-31</td>\n",
       "      <td>IMlisacowan</td>\n",
       "      <td>6</td>\n",
       "      <td>sunshin livingg ittt imma lie on the grass li...</td>\n",
       "      <td>2009-05-31 6  sunshin livingg ittt imma lie on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-06-01</td>\n",
       "      <td>yaseminx3</td>\n",
       "      <td>11</td>\n",
       "      <td>someth for your iphon</td>\n",
       "      <td>2009-06-01 11  someth for your iphon yaseminx3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-05-17</td>\n",
       "      <td>no_surprises</td>\n",
       "      <td>2</td>\n",
       "      <td>couldn t get in to the after parti</td>\n",
       "      <td>2009-05-17 2  couldn t get in to the after par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-06-02</td>\n",
       "      <td>Rhi_ShortStack</td>\n",
       "      <td>0</td>\n",
       "      <td>a andi be mean again now i want macca</td>\n",
       "      <td>2009-06-02 0  a andi be mean again now i want ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment        date            user  date_hour  \\\n",
       "0          1  2009-05-18       Killandra          1   \n",
       "1          1  2009-05-31     IMlisacowan          6   \n",
       "2          1  2009-06-01       yaseminx3         11   \n",
       "3          0  2009-05-17    no_surprises          2   \n",
       "4          0  2009-06-02  Rhi_ShortStack          0   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0   ye talk help a lot go through it there s no j...   \n",
       "1   sunshin livingg ittt imma lie on the grass li...   \n",
       "2                              someth for your iphon   \n",
       "3                 couldn t get in to the after parti   \n",
       "4              a andi be mean again now i want macca   \n",
       "\n",
       "                                               total  \n",
       "0  2009-05-18 1  ye talk help a lot go through it...  \n",
       "1  2009-05-31 6  sunshin livingg ittt imma lie on...  \n",
       "2     2009-06-01 11  someth for your iphon yaseminx3  \n",
       "3  2009-05-17 2  couldn t get in to the after par...  \n",
       "4  2009-06-02 0  a andi be mean again now i want ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>date_hour</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-06-01</td>\n",
       "      <td>urbanperspectiv</td>\n",
       "      <td>21</td>\n",
       "      <td>i m pretti much the same in either world</td>\n",
       "      <td>2009-06-01 21  i m pretti much the same in eit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-05-17</td>\n",
       "      <td>therealsecret</td>\n",
       "      <td>11</td>\n",
       "      <td>same here have a gr week ahead</td>\n",
       "      <td>2009-05-17 11  same here have a gr week ahead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-04-19</td>\n",
       "      <td>bitchville</td>\n",
       "      <td>23</td>\n",
       "      <td>that s just nightmar all over</td>\n",
       "      <td>2009-04-19 23  that s just nightmar all over b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-06-16</td>\n",
       "      <td>epi_longo</td>\n",
       "      <td>0</td>\n",
       "      <td>ch c ph i i thi i h c th t qu</td>\n",
       "      <td>2009-06-16 0  ch c ph i i thi i h c th t qu ep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-05-30</td>\n",
       "      <td>Curiosafmmb</td>\n",
       "      <td>12</td>\n",
       "      <td>sweeti awe ok sweeti ttyl hug</td>\n",
       "      <td>2009-05-30 12  sweeti awe ok sweeti ttyl hug C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date             user  date_hour  \\\n",
       "0  2009-06-01  urbanperspectiv         21   \n",
       "1  2009-05-17    therealsecret         11   \n",
       "2  2009-04-19       bitchville         23   \n",
       "3  2009-06-16        epi_longo          0   \n",
       "4  2009-05-30      Curiosafmmb         12   \n",
       "\n",
       "                                  clean_text  \\\n",
       "0   i m pretti much the same in either world   \n",
       "1             same here have a gr week ahead   \n",
       "2              that s just nightmar all over   \n",
       "3              ch c ph i i thi i h c th t qu   \n",
       "4              sweeti awe ok sweeti ttyl hug   \n",
       "\n",
       "                                               total  \n",
       "0  2009-06-01 21  i m pretti much the same in eit...  \n",
       "1  2009-05-17 11  same here have a gr week ahead ...  \n",
       "2  2009-04-19 23  that s just nightmar all over b...  \n",
       "3  2009-06-16 0  ch c ph i i thi i h c th t qu ep...  \n",
       "4  2009-05-30 12  sweeti awe ok sweeti ttyl hug C...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1 (SGDClassifier algorithm) on Prep 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting of the dataset in training and test set fixing the test size of 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(dev_df['total'], dev_df['sentiment'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt of the algorithm SGDClassifier without tuning the hyperparameters\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(eval_df['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data= predicted, columns=['Predicted'])\n",
    "df.insert(0,'Id',df.index)\n",
    "df\n",
    "df.to_csv('C:/Users/LENOVO/Downloads/OutputSGDprep5notuning.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 864 candidates, totalling 2592 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 40.6min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 74.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 115.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 169.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 231.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2592 out of 2592 | elapsed: 245.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.842\n"
     ]
    }
   ],
   "source": [
    "#application of the GridSearchCV for finding the best combination of hyperparameters for all the three algorithms\n",
    "#CountVectorizer(max_df, max_features, ngram_range)\n",
    "#TfidfTransform (use_idf)\n",
    "#SGDClassifier (max_iter, alpha, penalty)\n",
    "\n",
    "parameters = {\n",
    "    \"vect__max_df\": (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3)),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    \"clf__loss\": ('hinge', 'log'),\n",
    "    \"clf__alpha\": (0.00001, 0.000001),\n",
    "    'clf__max_iter': (1000, 5000, 10000),\n",
    "}\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(text_clf, parameters, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 1e-06\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__max_iter: 5000\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: None\n",
      "\tvect__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "#Printing the best combination of hyperparameters\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt of the SGDClassifier after the GridSearch\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.5, ngram_range=(1, 3), max_features=None)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', SGDClassifier(alpha=1e-06, max_iter=5000, loss='hinge')),\n",
    "])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(eval_df['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data= predicted, columns=['Predicted'])\n",
    "df.insert(0,'Id',df.index)\n",
    "df\n",
    "df.to_csv('C:/Users/LENOVO/Downloads/OutputSGDprep5.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2 (MultinomialNB algorithm) on Prep 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting of the dataset in training and test set fixing the test size of 0.01\n",
    "X_train, X_test, y_train, y_test = train_test_split(dev_df['total'], dev_df['sentiment'], test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attempt of the MultinomialNB without hyperparameters tuning\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(eval_df['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data= predicted, columns=['Predicted'])\n",
    "df.insert(0,'Id',df.index)\n",
    "df\n",
    "df.to_csv('C:/Users/LENOVO/Downloads/OutputNBprep5notuning.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.830\n"
     ]
    }
   ],
   "source": [
    "#application of the GridSearchCV for finding the best combination of hyperparameters for all the three algorithms\n",
    "#CountVectorizer(max_df, ngram_range)\n",
    "#TfidfTransform\n",
    "#SGDClassifier (alpha)\n",
    "\n",
    "parameters = {\n",
    "    \"vect__max_df\": (0.5, 0.75, 1.0),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3)),\n",
    "    'clf__alpha': np.arange(0, 1, 0.05),\n",
    "}\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(text_clf, parameters, n_jobs=-1, cv=3)\n",
    "grid_search.fit(dev_df['total'], dev_df['sentiment'])\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.15000000000000002\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "#Printing the best combination of hyperparameters\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt of the MultinomialNB after the GridSearch\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), max_df=0.5)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB(alpha=0.15000000000000002)),\n",
    "])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(eval_df['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data= predicted, columns=['Predicted'])\n",
    "df.insert(0,'Id',df.index)\n",
    "df\n",
    "df.to_csv('C:/Users/LENOVO/Downloads/OutputNBprep5.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
